We examined the training performance of the DistilBERT model on the
Stanford Sentiment Treebank-2 dataset, focusing on training times across
different platforms and comparing the efficiency of various training
environments.

Results:
● GPU: 92.999 s (around 1.5 minutes)
● Cluster: 1155.50 s (around 19 min)
● Colab: 9673.45 s (161 min)
● Colab GPU: 140.006 (around 2 min 20 secs)
● Mac M3 Max: 112.59 s (around 2 min)

